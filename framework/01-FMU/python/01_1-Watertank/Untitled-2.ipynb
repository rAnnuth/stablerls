{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ray.shutdown()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "from ray.rllib.algorithms import sac\n",
    "from ray.rllib.algorithms import pg\n",
    "from ray.rllib.algorithms import a3c\n",
    "from ray.rllib.algorithms import td3\n",
    "\n",
    "ray.init()\n",
    "\n",
    "\n",
    "import os\n",
    "import fmuSimulation.gymFMU as ExampleFMU\n",
    "from fmuSimulation.configReader import configReader\n",
    "import numpy as np\n",
    "import sys\n",
    "from torchModel import customTorchModel, FullyConnectedNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "config = os.path.abspath('Example.cfg')\n",
    "cfg = configReader(config)\n",
    "config = cfg.getAgent()\n",
    "#if 0:   \n",
    "config['framework'] = 'torch'\n",
    "    #agent = ppo.PPO(env=ExampleFMU.gymFMU, config={\"env_config\": config, \"num_workers\": 1, \"num_gpus\": 0})\n",
    "config['model'] = {}\n",
    "config['model']['custom_model'] = FullyConnectedNetwork\n",
    "config['model']['custom_model'] = customTorchModel\n",
    "#config['model']['fcnet_hiddens'] = [10]\n",
    "#\n",
    "# config['model']['no_final_linear'] = False\n",
    "#config['model']['custom_model'] = MyKerasModel\n",
    "agent = pg.PG(env=ExampleFMU.gymFMU, config=config)\n",
    "\n",
    "#agent = td3.TD3(env=ExampleFMU.gymFMU, config=config)\n",
    "#agent = sac.SAC(env=ExampleFMU.gymFMU, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'observation_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[39m=\u001b[39m MyAlgo(observation_space\u001b[39m=\u001b[39;49m[\u001b[39m2\u001b[39;49m])\n",
      "File \u001b[0;32m~/git/env/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:414\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    407\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m     }\n\u001b[1;32m    412\u001b[0m }\n\u001b[0;32m--> 414\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config\u001b[39m=\u001b[39;49mconfig, logger_creator\u001b[39m=\u001b[39;49mlogger_creator, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    416\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'observation_space'"
     ]
    }
   ],
   "source": [
    "x = MyAlgo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.policy.torch_policy_template import build_torch_policy\n",
    "\n",
    "class CustomPolicy(Policy):\n",
    "    \"\"\"Example of a custom policy written from scratch.\n",
    "\n",
    "    You might find it more convenient to use the `build_tf_policy` and\n",
    "    `build_torch_policy` helpers instead for a real policy, which are\n",
    "    described in the next sections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        Policy.__init__(self, observation_space, action_space, config)\n",
    "        # example parameter\n",
    "        self.w = 1.0\n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        # return action batch, RNN states, extra values to include in batch\n",
    "        return [self.action_space.sample() for _ in obs_batch], [], {}\n",
    "\n",
    "    def learn_on_batch(self, samples):\n",
    "        # implement your learning code here\n",
    "        return {}  # return stats\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {\"w\": self.w}\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.w = weights[\"w\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ray.rllib.algorithms.pg.pg' has no attribute 'with_updates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [82], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[39mreturn\u001b[39;00m PGTorchPolicy\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39malgorithms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpg\u001b[39;00m \u001b[39mimport\u001b[39;00m pg\n\u001b[0;32m---> 14\u001b[0m test \u001b[39m=\u001b[39m pg\u001b[39m.\u001b[39;49mwith_updates(default_policy\u001b[39m=\u001b[39mTestalg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'ray.rllib.algorithms.pg.pg' has no attribute 'with_updates'"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "class MyAlgo(Algorithm):\n",
    "    def get_default_policy_class(self, config):\n",
    "        return PGTorchPolicy\n",
    "\n",
    "\n",
    "class Testalg(pg.PG):\n",
    "    @classmethod\n",
    "    @override(Algorithm)\n",
    "    def get_default_policy_class(cls, config):\n",
    "        return PGTorchPolicy\n",
    "\n",
    "from ray.rllib.algorithms.pg import pg\n",
    "test = pg.with_updates(default_policy=Testalg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'PGConfig' has no attribute 'from_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [81], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39malgorithms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpg\u001b[39;00m \u001b[39mimport\u001b[39;00m pg\n\u001b[1;32m      2\u001b[0m \u001b[39m#config = {}\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#config['model'] = {}\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#config['model']['custom_model'] = PGTorchPolicy\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m Testalg(env\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mCartPole-v1\u001b[39;49m\u001b[39m'\u001b[39;49m, config\u001b[39m=\u001b[39;49mcfg)\n\u001b[1;32m      6\u001b[0m \u001b[39m#tune.Tuner(MyAlgo, param_space={\"env\": \"CartPole-v1\", \"num_workers\": 2}).fit()\u001b[39;00m\n",
      "File \u001b[0;32m~/git/env/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:414\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    407\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m     }\n\u001b[1;32m    412\u001b[0m }\n\u001b[0;32m--> 414\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config\u001b[39m=\u001b[39;49mconfig, logger_creator\u001b[39m=\u001b[39;49mlogger_creator, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    416\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/git/env/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:161\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout)\u001b[0m\n\u001b[1;32m    159\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 161\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    162\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/git/env/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:524\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39mif\u001b[39;00m _init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     \u001b[39m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    520\u001b[0m     \u001b[39m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    525\u001b[0m             env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    526\u001b[0m             validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    527\u001b[0m             policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    528\u001b[0m             trainer_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    529\u001b[0m             num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    530\u001b[0m             local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    531\u001b[0m             logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    532\u001b[0m         )\n\u001b[1;32m    533\u001b[0m     \u001b[39m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    534\u001b[0m     \u001b[39m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[39m# constructor).\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    537\u001b[0m         \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    538\u001b[0m         \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    539\u001b[0m         \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    540\u001b[0m         \u001b[39m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/git/env/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:185\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    182\u001b[0m     spaces \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m local_worker:\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_worker \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_worker(\n\u001b[1;32m    186\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49mRolloutWorker,\n\u001b[1;32m    187\u001b[0m         env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[1;32m    188\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    189\u001b[0m         policy_cls\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_class,\n\u001b[1;32m    190\u001b[0m         \u001b[39m# Initially, policy_specs will be inferred from config dict.\u001b[39;49;00m\n\u001b[1;32m    191\u001b[0m         policy_specs\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    192\u001b[0m         worker_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    193\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    194\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_local_config,\n\u001b[1;32m    195\u001b[0m         spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[1;32m    196\u001b[0m     )\n",
      "File \u001b[0;32m~/git/env/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:892\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[0;34m(self, cls, env_creator, validate_env, policy_cls, policy_specs, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     extra_python_environs \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mextra_python_environs_for_worker\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 892\u001b[0m worker \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    893\u001b[0m     env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[1;32m    894\u001b[0m     validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    895\u001b[0m     policy_spec\u001b[39m=\u001b[39;49mpolicy_specs,\n\u001b[1;32m    896\u001b[0m     policy_mapping_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicy_mapping_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    897\u001b[0m     policies_to_train\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicies_to_train\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    898\u001b[0m     tf_session_creator\u001b[39m=\u001b[39;49m(session_creator \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mtf_session_args\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    899\u001b[0m     rollout_fragment_length\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrollout_fragment_length\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    900\u001b[0m     count_steps_by\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mcount_steps_by\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    901\u001b[0m     batch_mode\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_mode\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    902\u001b[0m     episode_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mhorizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    903\u001b[0m     preprocessor_pref\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mpreprocessor_pref\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    904\u001b[0m     sample_async\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msample_async\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    905\u001b[0m     compress_observations\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcompress_observations\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    906\u001b[0m     num_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_envs_per_worker\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    907\u001b[0m     observation_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mobservation_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    908\u001b[0m     clip_rewards\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_rewards\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    909\u001b[0m     normalize_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnormalize_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    910\u001b[0m     clip_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    911\u001b[0m     env_config\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39menv_config\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    912\u001b[0m     policy_config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    913\u001b[0m     worker_index\u001b[39m=\u001b[39;49mworker_index,\n\u001b[1;32m    914\u001b[0m     num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    915\u001b[0m     recreated_worker\u001b[39m=\u001b[39;49mrecreated_worker,\n\u001b[1;32m    916\u001b[0m     log_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logdir,\n\u001b[1;32m    917\u001b[0m     log_level\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mlog_level\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    918\u001b[0m     callbacks\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    919\u001b[0m     input_creator\u001b[39m=\u001b[39;49minput_creator,\n\u001b[1;32m    920\u001b[0m     output_creator\u001b[39m=\u001b[39;49moutput_creator,\n\u001b[1;32m    921\u001b[0m     remote_worker_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_worker_envs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    922\u001b[0m     remote_env_batch_wait_ms\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_env_batch_wait_ms\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    923\u001b[0m     soft_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msoft_horizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    924\u001b[0m     no_done_at_end\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mno_done_at_end\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    925\u001b[0m     seed\u001b[39m=\u001b[39;49m(config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m+\u001b[39;49m worker_index)\n\u001b[1;32m    926\u001b[0m     \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    927\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    928\u001b[0m     fake_sampler\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mfake_sampler\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    929\u001b[0m     extra_python_environs\u001b[39m=\u001b[39;49mextra_python_environs,\n\u001b[1;32m    930\u001b[0m     spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[1;32m    931\u001b[0m     disable_env_checking\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mdisable_env_checking\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    932\u001b[0m )\n\u001b[1;32m    934\u001b[0m \u001b[39mreturn\u001b[39;00m worker\n",
      "File \u001b[0;32m~/git/env/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:625\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[0;34m(self, env_creator, validate_env, policy_spec, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, clip_rewards, normalize_actions, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, recreated_worker, log_dir, log_level, callbacks, input_creator, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler, spaces, policy, disable_env_checking)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m     ray\u001b[39m.\u001b[39mis_initialized()\n\u001b[1;32m    614\u001b[0m     \u001b[39mand\u001b[39;00m ray\u001b[39m.\u001b[39m_private\u001b[39m.\u001b[39mworker\u001b[39m.\u001b[39m_mode() \u001b[39m==\u001b[39m ray\u001b[39m.\u001b[39m_private\u001b[39m.\u001b[39mworker\u001b[39m.\u001b[39mLOCAL_MODE\n\u001b[1;32m    615\u001b[0m     \u001b[39mand\u001b[39;00m num_gpus \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    616\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m policy_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_fake_gpus\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    617\u001b[0m ):\n\u001b[1;32m    618\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    619\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are running ray with `local_mode=True`, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    620\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfigured \u001b[39m\u001b[39m{\u001b[39;00mnum_gpus\u001b[39m}\u001b[39;00m\u001b[39m GPUs to be used! In local mode, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    621\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPolicies are placed on the CPU and the `num_gpus` setting \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    622\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis ignored.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    623\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_policy_map(\n\u001b[1;32m    626\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_dict,\n\u001b[1;32m    627\u001b[0m     policy_config,\n\u001b[1;32m    628\u001b[0m     session_creator\u001b[39m=\u001b[39;49mtf_session_creator,\n\u001b[1;32m    629\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    630\u001b[0m )\n\u001b[1;32m    632\u001b[0m \u001b[39m# Update Policy's view requirements from Model, only if Policy directly\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[39m# inherited from base `Policy` class. At this point here, the Policy\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[39m# must have it's Model (if any) defined and ready to output an initial\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[39m# state.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[39mfor\u001b[39;00m pol \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/git/env/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:1899\u001b[0m, in \u001b[0;36mRolloutWorker._build_policy_map\u001b[0;34m(self, policy_dict, policy_config, policy, session_creator, seed)\u001b[0m\n\u001b[1;32m   1896\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map\u001b[39m.\u001b[39minsert_policy(name, policy)\n\u001b[1;32m   1897\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1898\u001b[0m     \u001b[39m# Create the actual policy object.\u001b[39;00m\n\u001b[0;32m-> 1899\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_map\u001b[39m.\u001b[39;49mcreate_policy(\n\u001b[1;32m   1900\u001b[0m         name,\n\u001b[1;32m   1901\u001b[0m         policy_spec\u001b[39m.\u001b[39;49mpolicy_class,\n\u001b[1;32m   1902\u001b[0m         obs_space,\n\u001b[1;32m   1903\u001b[0m         policy_spec\u001b[39m.\u001b[39;49maction_space,\n\u001b[1;32m   1904\u001b[0m         policy_spec\u001b[39m.\u001b[39;49mconfig,  \u001b[39m# overrides.\u001b[39;49;00m\n\u001b[1;32m   1905\u001b[0m         merged_conf,\n\u001b[1;32m   1906\u001b[0m     )\n\u001b[1;32m   1908\u001b[0m \u001b[39mif\u001b[39;00m connectors_enabled \u001b[39mand\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map:\n\u001b[1;32m   1909\u001b[0m     create_connectors_for_policy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map[name], policy_config)\n",
      "File \u001b[0;32m~/git/env/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py:134\u001b[0m, in \u001b[0;36mPolicyMap.create_policy\u001b[0;34m(self, policy_id, policy_cls, observation_space, action_space, config_override, merged_config)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\"\"\"Creates a new policy and stores it to the cache.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m        default config + `config_override`).\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m _class \u001b[39m=\u001b[39m get_tf_eager_cls_if_necessary(policy_cls, merged_config)\n\u001b[0;32m--> 134\u001b[0m policy \u001b[39m=\u001b[39m create_policy_for_framework(\n\u001b[1;32m    135\u001b[0m     policy_id,\n\u001b[1;32m    136\u001b[0m     _class,\n\u001b[1;32m    137\u001b[0m     merged_config,\n\u001b[1;32m    138\u001b[0m     observation_space,\n\u001b[1;32m    139\u001b[0m     action_space,\n\u001b[1;32m    140\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworker_index,\n\u001b[1;32m    141\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession_creator,\n\u001b[1;32m    142\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m    143\u001b[0m )\n\u001b[1;32m    144\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minsert_policy(policy_id, policy, config_override)\n",
      "File \u001b[0;32m~/git/env/lib/python3.9/site-packages/ray/rllib/utils/policy.py:107\u001b[0m, in \u001b[0;36mcreate_policy_for_framework\u001b[0;34m(policy_id, policy_class, merged_config, observation_space, action_space, worker_index, session_creator, seed)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 tf1\u001b[39m.\u001b[39mset_random_seed(seed)\n\u001b[1;32m    106\u001b[0m             \u001b[39mwith\u001b[39;00m tf1\u001b[39m.\u001b[39mvariable_scope(var_scope):\n\u001b[0;32m--> 107\u001b[0m                 \u001b[39mreturn\u001b[39;00m policy_class(\n\u001b[1;32m    108\u001b[0m                     observation_space, action_space, merged_config\n\u001b[1;32m    109\u001b[0m                 )\n\u001b[1;32m    110\u001b[0m \u001b[39m# For tf-eager: no graph, no session.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[39mwith\u001b[39;00m tf1\u001b[39m.\u001b[39mvariable_scope(var_scope):\n",
      "Cell \u001b[0;32mIn [80], line 27\u001b[0m, in \u001b[0;36mPGTorchPolicy.__init__\u001b[0;34m(self, observation_space, action_space, config)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, observation_space, action_space, config: PGConfig):\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m     \u001b[39m# Enforce AlgorithmConfig for PG Policies.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, \u001b[39mdict\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m         config \u001b[39m=\u001b[39m PGConfig\u001b[39m.\u001b[39;49mfrom_dict(config)\n\u001b[1;32m     28\u001b[0m         \u001b[39m#pass\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     TorchPolicyV2\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m     30\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m     31\u001b[0m         observation_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m         max_seq_len\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mmodel[\u001b[39m\"\u001b[39m\u001b[39mmax_seq_len\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     35\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'PGConfig' has no attribute 'from_dict'"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.pg import pg\n",
    "#config = {}\n",
    "#config['model'] = {}\n",
    "#config['model']['custom_model'] = PGTorchPolicy\n",
    "Testalg(env='CartPole-v1', config=cfg)\n",
    "#tune.Tuner(MyAlgo, param_space={\"env\": \"CartPole-v1\", \"num_workers\": 2}).fit()\n",
    "num_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Type, Union, Optional, Tuple\n",
    "\n",
    "from ray.rllib.evaluation.episode import Episode\n",
    "from ray.rllib.utils.typing import AgentID\n",
    "from ray.rllib.policy.torch_policy_v2 import TorchPolicyV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.numpy import convert_to_numpy\n",
    "from ray.rllib.algorithms.pg.pg import PGConfig\n",
    "from ray.rllib.algorithms.pg.utils import post_process_advantages\n",
    "from ray.rllib.evaluation.postprocessing import Postprocessing\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchDistributionWrapper\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.policy.torch_mixins import LearningRateSchedule\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "\n",
    "class PGTorchPolicy(LearningRateSchedule, TorchPolicyV2):\n",
    "    \"\"\"PyTorch policy class used with PGTrainer.\"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, config: PGConfig):\n",
    "\n",
    "        # Enforce AlgorithmConfig for PG Policies.\n",
    "        if isinstance(config, dict):\n",
    "            config = PGConfig.from_dict(config)\n",
    "            #pass\n",
    "        TorchPolicyV2.__init__(\n",
    "            self,\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            config,\n",
    "            max_seq_len=config.model[\"max_seq_len\"],\n",
    "        )\n",
    "\n",
    "        LearningRateSchedule.__init__(self, config.lr, config.lr_schedule)\n",
    "\n",
    "        # TODO: Don't require users to call this manually.\n",
    "        self._initialize_loss_from_dummy_batch()\n",
    "\n",
    "    @override(TorchPolicyV2)\n",
    "    def loss(\n",
    "        self,\n",
    "        model: ModelV2,\n",
    "        dist_class: Type[TorchDistributionWrapper],\n",
    "        train_batch: SampleBatch,\n",
    "    ) -> Union[TensorType, List[TensorType]]:\n",
    "        \"\"\"The basic policy gradients loss function.\n",
    "        Calculates the vanilla policy gradient loss based on:\n",
    "        L = -E[ log(pi(a|s)) * A]\n",
    "        Args:\n",
    "            model: The Model to calculate the loss for.\n",
    "            dist_class: The action distr. class.\n",
    "            train_batch: The training data.\n",
    "        Returns:\n",
    "            Union[TensorType, List[TensorType]]: A single loss tensor or a list\n",
    "                of loss tensors.\n",
    "        \"\"\"\n",
    "        # Pass the training data through our model to get distribution parameters.\n",
    "        dist_inputs, _ = model(train_batch)\n",
    "\n",
    "        # Create an action distribution object.\n",
    "        action_dist = dist_class(dist_inputs, model)\n",
    "\n",
    "        # Calculate the vanilla PG loss based on:\n",
    "        # L = -E[ log(pi(a|s)) * A]\n",
    "        log_probs = action_dist.logp(train_batch[SampleBatch.ACTIONS])\n",
    "\n",
    "        # Final policy loss.\n",
    "        policy_loss = -torch.mean(log_probs * train_batch[Postprocessing.ADVANTAGES])\n",
    "\n",
    "        # Store values for stats function in model (tower), such that for\n",
    "        # multi-GPU, we do not override them during the parallel loss phase.\n",
    "        model.tower_stats[\"policy_loss\"] = policy_loss\n",
    "\n",
    "        return policy_loss\n",
    "\n",
    "    @override(TorchPolicyV2)\n",
    "    def stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n",
    "        \"\"\"Returns the calculated loss in a stats dict.\n",
    "        Args:\n",
    "            policy: The Policy object.\n",
    "            train_batch: The data used for training.\n",
    "        Returns:\n",
    "            Dict[str, TensorType]: The stats dict.\n",
    "        \"\"\"\n",
    "\n",
    "        return convert_to_numpy(\n",
    "            {\n",
    "                \"policy_loss\": torch.mean(\n",
    "                    torch.stack(self.get_tower_stats(\"policy_loss\"))\n",
    "                ),\n",
    "                \"cur_lr\": self.cur_lr,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @override(TorchPolicyV2)\n",
    "    def postprocess_trajectory(\n",
    "        self,\n",
    "        sample_batch: SampleBatch,\n",
    "        other_agent_batches: Optional[\n",
    "            Dict[AgentID, Tuple[\"Policy\", SampleBatch]]\n",
    "        ] = None,\n",
    "        episode: Optional[\"Episode\"] = None,\n",
    "    ) -> SampleBatch:\n",
    "        sample_batch = super().postprocess_trajectory(\n",
    "            sample_batch, other_agent_batches, episode\n",
    "        )\n",
    "        return post_process_advantages(self, sample_batch, other_agent_batches, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a7e96d51479e192f437693b69736ebafa6e31b8e0eb2319a92624b7e7440c0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
