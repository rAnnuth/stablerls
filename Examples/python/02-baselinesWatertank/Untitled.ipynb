{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b44f840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 13:51:34,297\tINFO worker.py:1528 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.9.12</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.1.0</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.9.12', ray_version='2.1.0', ray_commit='be49bde7ee4f6adb3f8710aee0665c27f9f0bb62', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': 'tcp://127.0.0.1:56474', 'raylet_socket_name': 'tcp://127.0.0.1:62155', 'webui_url': '', 'session_dir': 'C:\\\\Users\\\\Robert\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2022-11-17_13-51-32_771868_18840', 'metrics_export_port': 62063, 'gcs_address': '127.0.0.1:63924', 'address': '127.0.0.1:63924', 'dashboard_agent_listen_port': 52365, 'node_id': 'b33b325304e8971f4dc8ce4e836ab993b80dc3092e50405e42c1a857'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    ray.shutdown()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "from ray.rllib.algorithms import sac\n",
    "from ray.rllib.algorithms import pg\n",
    "from ray.rllib.algorithms import a3c\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3269bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 13:51:41,669\tINFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No function for resetIO implemented\n",
      "No function for obsProcessing implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39152)\u001b[0m No function for resetIO implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39152)\u001b[0m No function for obsProcessing implemented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=39152)\u001b[0m Incompatible sample time and stop time.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39152)\u001b[0m  Using 200.0 as stop time instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=39152)\u001b[0m 2022-11-17 13:51:45,102\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fmuSimulation.gymFMU as ExampleFMU\n",
    "from fmuSimulation.configReader import configReader\n",
    "import numpy as np\n",
    "import sys\n",
    "#from torchModel import customTorchModel, FullyConnectedNetwork\n",
    "from ExampleTorch import FullyConnectedNetwork\n",
    "#from ExampleTF import Keras_FullyConnectedNetwork as fcTF\n",
    "\n",
    "config = os.path.abspath('Example.cfg')\n",
    "cfg = configReader(config)\n",
    "config = cfg.getAgent()\n",
    "#if 0:   \n",
    "config['framework'] = 'torch'\n",
    "    #agent = ppo.PPO(env=ExampleFMU.gymFMU, config={\"env_config\": config, \"num_workers\": 1, \"num_gpus\": 0})\n",
    "config['model'] = {}\n",
    "#config['model']['custom_model'] = FullyConnectedNetwork\n",
    "config['model']['fcnet_hiddens'] = [10]\n",
    "config['model']['no_final_linear'] = False\n",
    "#config['model']['custom_model'] = MyKerasModel\n",
    "agent = a3c.A3C(env=ExampleFMU.gymFMU, config=config)\n",
    "#agent = ppo.PPO(env=ExampleFMU.gymFMU, config=config)\n",
    "#agent = sac.SAC(env=ExampleFMU.gymFMU, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e8f58ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=23356)\u001b[0m No function for resetIO implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23356)\u001b[0m No function for obsProcessing implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43888)\u001b[0m No function for resetIO implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43888)\u001b[0m No function for obsProcessing implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8548)\u001b[0m No function for resetIO implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8548)\u001b[0m No function for obsProcessing implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23356)\u001b[0m Incompatible sample time and stop time.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23356)\u001b[0m  Using 200.0 as stop time instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43888)\u001b[0m Incompatible sample time and stop time.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43888)\u001b[0m  Using 200.0 as stop time instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8548)\u001b[0m Incompatible sample time and stop time.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8548)\u001b[0m  Using 200.0 as stop time instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30056)\u001b[0m No function for resetIO implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30056)\u001b[0m No function for obsProcessing implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30056)\u001b[0m Incompatible sample time and stop time.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30056)\u001b[0m  Using 200.0 as stop time instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44504)\u001b[0m No function for resetIO implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44504)\u001b[0m No function for obsProcessing implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44504)\u001b[0m Incompatible sample time and stop time.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44504)\u001b[0m  Using 200.0 as stop time instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20928)\u001b[0m No function for resetIO implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20928)\u001b[0m No function for obsProcessing implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20928)\u001b[0m Incompatible sample time and stop time.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20928)\u001b[0m  Using 200.0 as stop time instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=44504)\u001b[0m 2022-11-17 14:27:57,075\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=33960)\u001b[0m No function for resetIO implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=33960)\u001b[0m No function for obsProcessing implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2080)\u001b[0m No function for resetIO implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2080)\u001b[0m No function for obsProcessing implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=33960)\u001b[0m Incompatible sample time and stop time.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=33960)\u001b[0m  Using 200.0 as stop time instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2080)\u001b[0m Incompatible sample time and stop time.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2080)\u001b[0m  Using 200.0 as stop time instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8656)\u001b[0m No function for resetIO implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8656)\u001b[0m No function for obsProcessing implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8656)\u001b[0m Incompatible sample time and stop time.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8656)\u001b[0m  Using 200.0 as stop time instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 14:27:57,776\tINFO trainable.py:164 -- Trainable.setup took 10.459 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=39784)\u001b[0m No function for resetIO implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39784)\u001b[0m No function for obsProcessing implemented\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39784)\u001b[0m Incompatible sample time and stop time.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=39784)\u001b[0m  Using 200.0 as stop time instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'num_recreated_workers': 0,\n",
       " 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 40.0,\n",
       "     'cur_lr': 0.0001,\n",
       "     'entropy_coeff': 0.01,\n",
       "     'policy_entropy': 23.786780039469402,\n",
       "     'policy_loss': -45494120448.0,\n",
       "     'vf_loss': 2.0821113045612757e+19}}},\n",
       "  'num_env_steps_sampled': 39750,\n",
       "  'num_env_steps_trained': 39750,\n",
       "  'num_agent_steps_sampled': 39750,\n",
       "  'num_agent_steps_trained': 39750},\n",
       " 'sampler_results': {'episode_reward_max': -676821688539.9546,\n",
       "  'episode_reward_min': -680564693277.0797,\n",
       "  'episode_reward_mean': -679271647559.991,\n",
       "  'episode_len_mean': 2000.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 15,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [-677842509282.8182,\n",
       "    -680564693277.0797,\n",
       "    -678863315911.1403,\n",
       "    -680564693277.0797,\n",
       "    -678523042111.8182,\n",
       "    -680224414874.124,\n",
       "    -679884142516.1705,\n",
       "    -680224411240.3926,\n",
       "    -676821688539.9546,\n",
       "    -680224412715.6787,\n",
       "    -678523019359.1814,\n",
       "    -679543863827.5967,\n",
       "    -678863323914.97,\n",
       "    -677842489274.7788,\n",
       "    -680564693277.0797],\n",
       "   'episode_lengths': [2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000,\n",
       "    2000]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.22084747718876602,\n",
       "   'mean_inference_ms': 0.8465604350004643,\n",
       "   'mean_action_processing_ms': 0.08889709139364764,\n",
       "   'mean_env_wait_ms': 0.09814642101524997,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0},\n",
       " 'episode_reward_max': -676821688539.9546,\n",
       " 'episode_reward_min': -680564693277.0797,\n",
       " 'episode_reward_mean': -679271647559.991,\n",
       " 'episode_len_mean': 2000.0,\n",
       " 'episodes_this_iter': 15,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'hist_stats': {'episode_reward': [-677842509282.8182,\n",
       "   -680564693277.0797,\n",
       "   -678863315911.1403,\n",
       "   -680564693277.0797,\n",
       "   -678523042111.8182,\n",
       "   -680224414874.124,\n",
       "   -679884142516.1705,\n",
       "   -680224411240.3926,\n",
       "   -676821688539.9546,\n",
       "   -680224412715.6787,\n",
       "   -678523019359.1814,\n",
       "   -679543863827.5967,\n",
       "   -678863323914.97,\n",
       "   -677842489274.7788,\n",
       "   -680564693277.0797],\n",
       "  'episode_lengths': [2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000,\n",
       "   2000]},\n",
       " 'sampler_perf': {'mean_raw_obs_processing_ms': 0.22084747718876602,\n",
       "  'mean_inference_ms': 0.8465604350004643,\n",
       "  'mean_action_processing_ms': 0.08889709139364764,\n",
       "  'mean_env_wait_ms': 0.09814642101524997,\n",
       "  'mean_env_render_ms': 0.0},\n",
       " 'num_faulty_episodes': 0,\n",
       " 'num_healthy_workers': 10,\n",
       " 'num_agent_steps_sampled': 39750,\n",
       " 'num_agent_steps_trained': 39750,\n",
       " 'num_env_steps_sampled': 39750,\n",
       " 'num_env_steps_trained': 39750,\n",
       " 'num_env_steps_sampled_this_iter': 39750,\n",
       " 'num_env_steps_trained_this_iter': 39750,\n",
       " 'timesteps_total': 39750,\n",
       " 'num_steps_trained_this_iter': 39750,\n",
       " 'agent_timesteps_total': 39750,\n",
       " 'timers': {'training_iteration_time_ms': 0.8,\n",
       "  'grad_wait_time_ms': 0.3,\n",
       "  'apply_grad_time_ms': 0.1,\n",
       "  'apply_grad_throughput': 99911.958,\n",
       "  'synch_weights_time_ms': 0.4},\n",
       " 'counters': {'num_env_steps_sampled': 39750,\n",
       "  'num_env_steps_trained': 39750,\n",
       "  'num_agent_steps_sampled': 39750,\n",
       "  'num_agent_steps_trained': 39750},\n",
       " 'done': False,\n",
       " 'episodes_total': 15,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'experiment_id': '644f9ba957a34631841a01f812e3757a',\n",
       " 'date': '2022-11-17_14-28-02',\n",
       " 'timestamp': 1668691682,\n",
       " 'time_this_iter_s': 5.009270429611206,\n",
       " 'time_total_s': 5.009270429611206,\n",
       " 'pid': 18840,\n",
       " 'hostname': 'DESKTOP-KC6I3RF',\n",
       " 'node_ip': '127.0.0.1',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'env': fmuSimulation.gymFMU.gymFMU,\n",
       "  'env_config': {'DEFAULT': {},\n",
       "   'General': {'DEBUG': False, 'LOG': False, 'Href': 10},\n",
       "   'FMU': {'fmuPath': '../../matlab/01-Watertank/rlwatertank.fmu',\n",
       "    'startTime': 0,\n",
       "    'stopTime': 200,\n",
       "    'tolerance': 1e-06,\n",
       "    'createFMU': False,\n",
       "    'dt': 0.1},\n",
       "   'Reinforcement Learning': {'actionInterval': 0.1},\n",
       "   'Environment': {},\n",
       "   'Reward': {}},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'disable_env_checking': False,\n",
       "  'num_workers': 10,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'sample_async': True,\n",
       "  'enable_connectors': False,\n",
       "  'rollout_fragment_length': 10,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'validate_workers_after_construction': True,\n",
       "  'ignore_worker_failures': False,\n",
       "  'recreate_failed_workers': False,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_worker_failures_tolerance': 100,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'compress_observations': False,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 0.0001,\n",
       "  'train_batch_size': 32,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [2],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': True,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': ExampleTorch.FullyConnectedNetwork,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1},\n",
       "  'optimizer': {},\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 180.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_config': {'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'placement_strategy': 'PACK',\n",
       "   'eager_tracing': False,\n",
       "   'eager_max_retraces': 20,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'env': fmuSimulation.gymFMU.gymFMU,\n",
       "   'env_config': {'DEFAULT': {},\n",
       "    'General': {'DEBUG': False, 'LOG': False, 'Href': 10},\n",
       "    'FMU': {'fmuPath': '../../matlab/01-Watertank/rlwatertank.fmu',\n",
       "     'startTime': 0,\n",
       "     'stopTime': 200,\n",
       "     'tolerance': 1e-06,\n",
       "     'createFMU': False,\n",
       "     'dt': 0.1},\n",
       "    'Reinforcement Learning': {'actionInterval': 0.1},\n",
       "    'Environment': {},\n",
       "    'Reward': {}},\n",
       "   'observation_space': None,\n",
       "   'action_space': None,\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'clip_rewards': None,\n",
       "   'normalize_actions': True,\n",
       "   'clip_actions': False,\n",
       "   'disable_env_checking': False,\n",
       "   'num_workers': 10,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'sample_async': True,\n",
       "   'enable_connectors': False,\n",
       "   'rollout_fragment_length': 10,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'validate_workers_after_construction': True,\n",
       "   'ignore_worker_failures': False,\n",
       "   'recreate_failed_workers': False,\n",
       "   'restart_failed_sub_environments': False,\n",
       "   'num_consecutive_worker_failures_tolerance': 100,\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'compress_observations': False,\n",
       "   'enable_tf1_exec_eagerly': False,\n",
       "   'sampler_perf_stats_ema_coef': None,\n",
       "   'gamma': 0.99,\n",
       "   'lr': 0.0001,\n",
       "   'train_batch_size': 32,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    '_disable_preprocessor_api': False,\n",
       "    '_disable_action_flattening': False,\n",
       "    'fcnet_hiddens': [2],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': True,\n",
       "    'vf_share_layers': True,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'framestack': True,\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': ExampleTorch.FullyConnectedNetwork,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1},\n",
       "   'optimizer': {},\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'StochasticSampling'},\n",
       "   'input_config': {},\n",
       "   'actions_in_input_normalized': False,\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_config': {},\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_duration': 10,\n",
       "   'evaluation_duration_unit': 'episodes',\n",
       "   'evaluation_sample_timeout_s': 180.0,\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'evaluation_config': {},\n",
       "   'off_policy_estimation_methods': {},\n",
       "   'ope_split_batch_by_episode': True,\n",
       "   'evaluation_num_workers': 0,\n",
       "   'always_attach_evaluation_results': False,\n",
       "   'enable_async_evaluation': False,\n",
       "   'in_evaluation': False,\n",
       "   'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "   'keep_per_episode_custom_metrics': False,\n",
       "   'metrics_episode_collection_timeout_s': 60.0,\n",
       "   'metrics_num_episodes_for_smoothing': 100,\n",
       "   'min_time_s_per_iteration': 5,\n",
       "   'min_train_timesteps_per_iteration': 0,\n",
       "   'min_sample_timesteps_per_iteration': 0,\n",
       "   'export_native_model_files': False,\n",
       "   'logger_creator': None,\n",
       "   'logger_config': None,\n",
       "   'log_level': 'WARN',\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'seed': None,\n",
       "   '_tf_policy_handles_more_than_one_loss': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   '_disable_execution_plan_api': True,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'evaluation_num_episodes': -1,\n",
       "   'metrics_smoothing_episodes': -1,\n",
       "   'timesteps_per_iteration': -1,\n",
       "   'min_iter_time_s': -1,\n",
       "   'collect_metrics_timeout': -1,\n",
       "   'buffer_size': -1,\n",
       "   'prioritized_replay': -1,\n",
       "   'learning_starts': -1,\n",
       "   'replay_batch_size': -1,\n",
       "   'replay_sequence_length': None,\n",
       "   'replay_mode': -1,\n",
       "   'prioritized_replay_alpha': -1,\n",
       "   'prioritized_replay_beta': -1,\n",
       "   'prioritized_replay_eps': -1,\n",
       "   'min_time_s_per_reporting': -1,\n",
       "   'min_train_timesteps_per_reporting': -1,\n",
       "   'min_sample_timesteps_per_reporting': -1,\n",
       "   'input_evaluation': -1,\n",
       "   'use_critic': True,\n",
       "   'use_gae': True,\n",
       "   'grad_clip': 40.0,\n",
       "   'lr_schedule': None,\n",
       "   'vf_loss_coeff': 0.5,\n",
       "   'entropy_coeff': 0.01,\n",
       "   'entropy_coeff_schedule': None,\n",
       "   'lambda': 1.0,\n",
       "   'input': 'sampler',\n",
       "   'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec at 0x23219c7fd00>},\n",
       "    'policy_map_capacity': 100,\n",
       "    'policy_map_cache': None,\n",
       "    'policy_mapping_fn': None,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "   'create_env_on_driver': False,\n",
       "   'custom_eval_function': None,\n",
       "   'framework': 'torch',\n",
       "   'num_cpus_for_driver': 1},\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_workers': 0,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'enable_async_evaluation': False,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': 5,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'export_native_model_files': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': True,\n",
       "  'simple_optimizer': False,\n",
       "  'monitor': -1,\n",
       "  'evaluation_num_episodes': -1,\n",
       "  'metrics_smoothing_episodes': -1,\n",
       "  'timesteps_per_iteration': -1,\n",
       "  'min_iter_time_s': -1,\n",
       "  'collect_metrics_timeout': -1,\n",
       "  'buffer_size': -1,\n",
       "  'prioritized_replay': -1,\n",
       "  'learning_starts': -1,\n",
       "  'replay_batch_size': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  'replay_mode': -1,\n",
       "  'prioritized_replay_alpha': -1,\n",
       "  'prioritized_replay_beta': -1,\n",
       "  'prioritized_replay_eps': -1,\n",
       "  'min_time_s_per_reporting': -1,\n",
       "  'min_train_timesteps_per_reporting': -1,\n",
       "  'min_sample_timesteps_per_reporting': -1,\n",
       "  'input_evaluation': -1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'grad_clip': 40.0,\n",
       "  'lr_schedule': None,\n",
       "  'vf_loss_coeff': 0.5,\n",
       "  'entropy_coeff': 0.01,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec at 0x23219965580>},\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': None,\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch',\n",
       "  'num_cpus_for_driver': 1},\n",
       " 'time_since_restore': 5.009270429611206,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 1,\n",
       " 'warmup_time': 10.461621046066284,\n",
       " 'perf': {'cpu_util_percent': 16.842857142857145,\n",
       "  'ram_util_percent': 48.60000000000001,\n",
       "  'gpu_util_percent0': 0.08142857142857143,\n",
       "  'vram_util_percent0': 0.055501302083333336}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.a3c import a3c\n",
    "#from a3c.a3c import a3c\n",
    "config['model'] = {}\n",
    "config['num_workers'] = 10\n",
    "from ExampleTorch import FullyConnectedNetwork\n",
    "config['model']['custom_model'] = FullyConnectedNetwork\n",
    "config['model']['fcnet_hiddens'] = [2]\n",
    "config['model']['no_final_linear'] = True\n",
    "\n",
    "agent2 = a3c.A3C(env=ExampleFMU.gymFMU, config=config)\n",
    "\n",
    "agent2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e13acc6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward -680564693277.08/-679050468963.77/-676141107985.46 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000002\n",
      "  2 reward -680564693277.08/-679005099631.87/-676141107985.46 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000003\n",
      "  3 reward -680564693277.08/-679054723683.86/-676141107985.46 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000004\n",
      "  4 reward -680564693277.08/-679087901011.48/-675800857440.80 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000005\n",
      "  5 reward -680564693277.08/-679128734253.39/-675800857440.80 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000006\n",
      "  6 reward -680564693277.08/-679217205886.46/-675800857440.80 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000007\n",
      "  7 reward -680564693277.08/-679332899702.05/-675800857440.80 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000008\n",
      "  8 reward -680564693277.08/-679298871852.26/-675460567321.52 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000009\n",
      "  9 reward -680564693277.08/-679305677043.58/-675460567321.52 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000010\n",
      " 10 reward -680564693277.08/-679349912972.47/-675460567321.52 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000011\n",
      " 11 reward -680564693277.08/-679469008900.02/-675460567321.52 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000012\n",
      " 12 reward -680564693277.08/-679489424931.64/-675460567321.52 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000013\n",
      " 13 reward -680564693277.08/-679649353324.99/-675460585719.69 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000014\n",
      " 14 reward -680564693277.08/-679696991866.57/-675460585719.69 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000015\n",
      " 15 reward -680564693277.08/-679754838700.57/-677502201479.89 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000016\n",
      " 16 reward -680564693277.08/-679778658360.26/-677502226602.72 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000017\n",
      " 17 reward -680564693277.08/-679788866506.81/-677502231584.47 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000018\n",
      " 18 reward -680564693277.08/-679748033843.05/-677502231584.47 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000019\n",
      " 19 reward -680564693277.08/-679802476957.33/-677161933100.88 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000020\n",
      " 20 reward -680564693277.08/-679717408240.00/-677161933100.88 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000021\n",
      " 21 reward -680564693277.08/-679625533722.57/-676821676359.38 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000022\n",
      " 22 reward -680564693277.08/-679581298698.06/-676481400856.03 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000023\n",
      " 23 reward -680564693277.08/-679608520616.22/-676481400856.03 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000024\n",
      " 24 reward -680564693277.08/-679577896216.76/-674780021699.02 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000025\n",
      " 25 reward -680564693277.08/-679584701372.79/-674780021699.02 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000026\n",
      " 26 reward -680564693277.08/-679571090082.63/-674780021699.02 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000027\n",
      " 27 reward -680564693277.08/-679526854237.78/-674780021699.02 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000028\n",
      " 28 reward -680564693277.08/-679523452065.66/-674780021699.02 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000029\n",
      " 29 reward -680564693277.08/-679472410697.49/-676481372219.71 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000030\n",
      " 30 reward -680564693277.08/-679526855479.97/-676481372219.71 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000031\n",
      " 31 reward -680564693277.08/-679608521302.30/-676481400261.05 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000032\n",
      " 32 reward -680564693277.08/-679659562644.33/-676481400261.05 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000033\n",
      " 33 reward -680564693277.08/-679662965108.52/-676481400261.05 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000034\n",
      " 34 reward -680564693277.08/-679662965606.97/-675460594804.42 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000035\n",
      " 35 reward -680564693277.08/-679662965328.26/-675460594804.42 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000036\n",
      " 36 reward -680564693277.08/-679693589987.41/-675460594804.42 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000037\n",
      " 37 reward -680564693277.08/-679632340664.77/-675460594804.42 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000038\n",
      " 38 reward -680564693277.08/-679618729950.80/-675460594804.42 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000039\n",
      " 39 reward -680564693277.08/-679707200505.04/-676481421991.96 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000040\n",
      " 40 reward -680564693277.08/-679768449769.75/-676481421991.96 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000041\n",
      " 41 reward -680564693277.08/-679761644492.39/-676481421991.96 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000042\n",
      " 42 reward -680564693277.08/-679782060505.30/-676141139241.83 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000043\n",
      " 43 reward -680564693277.08/-679792269119.11/-676141139241.83 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000044\n",
      " 44 reward -680564693277.08/-679741228235.86/-676141139241.83 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000045\n",
      " 45 reward -680564693277.08/-679632340548.07/-676141139241.83 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000046\n",
      " 46 reward -680564693277.08/-679560882231.14/-674780051689.99 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000047\n",
      " 47 reward -680564693277.08/-679601714918.65/-674780051689.99 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000048\n",
      " 48 reward -680564693277.08/-679489424000.25/-674780051689.99 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000049\n",
      " 49 reward -680564693277.08/-679383938169.41/-674780051689.99 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000050\n",
      " 50 reward -680564693277.08/-679380534915.64/-674780051689.99 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000051\n",
      " 51 reward -680564693277.08/-679288661186.80/-676481414329.47 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000052\n",
      " 52 reward -680564693277.08/-679196786435.50/-676481414329.47 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000053\n",
      " 53 reward -680564693277.08/-679210397458.88/-676481414329.47 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000054\n",
      " 54 reward -680564693277.08/-679159357486.41/-676481414329.47 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000055\n",
      " 55 reward -680564693277.08/-679176371468.09/-677161912399.66 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000056\n",
      " 56 reward -680564693277.08/-679210399319.04/-675120291033.82 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000057\n",
      " 57 reward -680564693277.08/-679210399337.46/-675120291033.82 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000058\n",
      " 58 reward -680564693277.08/-679322689563.51/-675120291033.82 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000059\n",
      " 59 reward -680564693277.08/-679475813142.31/-675120291033.82 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000060\n",
      " 60 reward -680564693277.08/-679560881666.04/-675120291033.82 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000061\n",
      " 61 reward -680564693277.08/-679714005540.21/-675460568021.20 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000062\n",
      " 62 reward -680564693277.08/-679771852729.81/-676821664356.38 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000063\n",
      " 63 reward -680564693277.08/-679686783775.91/-676821664356.38 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000064\n",
      " 64 reward -680564693277.08/-679622132070.74/-676821684998.59 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000065\n",
      " 65 reward -680564693277.08/-679554077422.42/-674780054998.44 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000066\n",
      " 66 reward -680564693277.08/-679479216869.44/-674780054998.44 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000067\n",
      " 67 reward -680564693277.08/-679567688338.15/-674780054998.44 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000068\n",
      " 68 reward -680564693277.08/-679608521438.06/-674780054998.44 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000069\n",
      " 69 reward -680564693277.08/-679601715803.18/-674780054998.44 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000070\n",
      " 70 reward -680564693277.08/-679717409373.86/-676141095554.81 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000071\n",
      " 71 reward -680564693277.08/-679802478010.16/-676141095554.81 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000072\n",
      " 72 reward -680564693277.08/-679761644967.80/-676141095554.81 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000073\n",
      " 73 reward -680564693277.08/-679748034039.82/-676481376202.55 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000074\n",
      " 74 reward -680564693277.08/-679802477512.91/-676481376202.55 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000075\n",
      " 75 reward -680564693277.08/-679744630172.80/-676481376202.55 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000076\n",
      " 76 reward -680564693277.08/-679778657502.02/-676481376202.55 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000077\n",
      " 77 reward -680564693277.08/-679826295333.60/-677502187846.77 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000078\n",
      " 78 reward -680564693277.08/-679785463075.42/-677161967816.01 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000079\n",
      " 79 reward -680564693277.08/-679809282846.72/-677161967816.01 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000080\n",
      " 80 reward -680564693277.08/-679822894015.55/-677161967816.01 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000081\n",
      " 81 reward -680564693277.08/-679799074990.90/-677161967816.01 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000082\n",
      " 82 reward -680564693277.08/-679761645040.11/-677161970777.94 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000083\n",
      " 83 reward -680564693277.08/-679744630998.93/-677161970777.94 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000084\n",
      " 84 reward -680564693277.08/-679618728922.04/-676141137167.37 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000085\n",
      " 85 reward -680564693277.08/-679662964467.87/-676141137167.37 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000086\n",
      " 86 reward -680564693277.08/-679666367819.66/-676141137167.37 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000087\n",
      " 87 reward -680564693277.08/-679700395246.72/-676141137167.37 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000088\n",
      " 88 reward -680564693277.08/-679737825669.30/-676141137167.37 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000089\n",
      " 89 reward -680564693277.08/-679765048758.54/-676821681849.38 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000090\n",
      " 90 reward -680564693277.08/-679731021450.61/-676821681849.38 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000091\n",
      " 91 reward -680564693277.08/-679734424293.63/-676821681849.38 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000092\n",
      " 92 reward -680564693277.08/-679686785567.65/-676821681849.38 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000093\n",
      " 93 reward -680564693277.08/-679669771372.57/-676821681849.38 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000094\n",
      " 94 reward -680564693277.08/-679659562659.48/-677161948983.12 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000095\n",
      " 95 reward -680564693277.08/-679628937583.61/-677161948983.12 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000096\n",
      " 96 reward -680564693277.08/-679642547995.02/-676481417648.34 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000097\n",
      " 97 reward -680564693277.08/-679594909557.42/-676481417648.34 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000098\n",
      " 98 reward -680564693277.08/-679543868134.01/-675460554858.42 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000099\n",
      " 99 reward -680564693277.08/-679445188860.30/-675460554858.42 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000100\n",
      "100 reward -680564693277.08/-679332898175.15/-675120319448.07 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000101\n",
      "101 reward -680564693277.08/-679373730596.85/-675120319448.07 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000102\n",
      "102 reward -680564693277.08/-679309078483.55/-675120319448.07 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000103\n",
      "103 reward -680564693277.08/-679281857131.42/-675120319448.07 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000104\n",
      "104 reward -680564693277.08/-679281856493.57/-675120319448.07 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000105\n",
      "105 reward -680564693277.08/-679322689246.02/-675460549158.41 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000106\n",
      "106 reward -680564693277.08/-679363522604.31/-675460549158.41 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000107\n",
      "107 reward -680564693277.08/-679469007853.22/-676141134220.74 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000108\n",
      "108 reward -680564693277.08/-679567687261.70/-676481380861.98 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000109\n",
      "109 reward -680564693277.08/-679724214860.33/-676481380861.98 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000110\n",
      "110 reward -680564693277.08/-679765048031.10/-676481380861.98 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000111\n",
      "111 reward -680564693277.08/-679785464622.05/-676481380861.98 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000112\n",
      "112 reward -680564693277.08/-679771853800.10/-675460596287.61 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000113\n",
      "113 reward -680564693277.08/-679870533130.84/-675460596287.61 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000114\n",
      "114 reward -680564693277.08/-679795672547.96/-675460596287.61 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000115\n",
      "115 reward -680564693277.08/-679788867333.01/-675460596287.61 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000116\n",
      "116 reward -680564693277.08/-679768451089.89/-675460596287.61 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000117\n",
      "117 reward -680564693277.08/-679785464568.25/-677161950610.93 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000118\n",
      "118 reward -680564693277.08/-679734423734.33/-676481426652.13 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000119\n",
      "119 reward -680564693277.08/-679748034904.89/-676481426652.13 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000120\n",
      "120 reward -680564693277.08/-679802478229.07/-676481426652.13 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000121\n",
      "121 reward -680564693277.08/-679724214589.17/-676481426652.13 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000122\n",
      "122 reward -680564693277.08/-679717409126.82/-676481426652.13 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000123\n",
      "123 reward -680564693277.08/-679758241601.95/-677502231022.75 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000124\n",
      "124 reward -680564693277.08/-679703797181.21/-676481406103.55 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000125\n",
      "125 reward -680564693277.08/-679724213984.09/-676481406103.55 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000126\n",
      "126 reward -680564693277.08/-679819491216.40/-676481406103.55 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000127\n",
      "127 reward -680564693277.08/-679850115895.64/-675800856278.88 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000128\n",
      "128 reward -680564693277.08/-679860324426.08/-674439761303.92 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000129\n",
      "129 reward -680564693277.08/-679846713712.74/-673418957473.98 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000130\n",
      "130 reward -680564693277.08/-679839907671.24/-673418957473.98 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000131\n",
      "131 reward -680564693277.08/-679805880890.29/-673418957473.98 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000132\n",
      "132 reward -680564693277.08/-679822894602.27/-673418957473.98 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000133\n",
      "133 reward -680564693277.08/-679833102788.65/-673418957473.98 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000134\n",
      "134 reward -680564693277.08/-679901157756.25/-677161950949.85 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000135\n",
      "135 reward -680564693277.08/-679979420976.10/-677161950949.85 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000136\n",
      "136 reward -680564693277.08/-679979420972.14/-677161950949.85 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000137\n",
      "137 reward -680564693277.08/-680050878764.51/-677161950949.85 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000138\n",
      "138 reward -680564693277.08/-680112128351.40/-677161971070.43 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000139\n",
      "139 reward -680564693277.08/-680139350221.93/-677161971070.43 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000140\n",
      "140 reward -680564693277.08/-680186988730.27/-677161971070.43 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000141\n",
      "141 reward -680564693277.08/-680210807746.85/-677161971070.43 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000142\n",
      "142 reward -680564693277.08/-680186988583.63/-678863320083.34 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000143\n",
      "143 reward -680564693277.08/-680210807337.93/-678863322136.02 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000144\n",
      "144 reward -680564693277.08/-680227820874.79/-679203596610.73 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000145\n",
      "145 reward -680564693277.08/-680255042790.83/-679203596610.73 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000146\n",
      "146 reward -680564693277.08/-680299278443.40/-679203596610.73 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000147\n",
      "147 reward -680564693277.08/-680363930219.65/-679203604408.46 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000148\n",
      "148 reward -680564693277.08/-680384347067.30/-679203604408.46 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000149\n",
      "149 reward -680564693277.08/-680428582904.86/-679203604408.46 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000150\n",
      "150 reward -680564693277.08/-680455804688.89/-679203604408.46 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000151\n",
      "151 reward -680564693277.08/-680486429461.03/-679884128779.21 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000152\n",
      "152 reward -680564693277.08/-680530665510.66/-679884128779.21 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000153\n",
      "153 reward -680564693277.08/-680551081996.19/-679884128779.21 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000154\n",
      "154 reward -680564693277.08/-680554484837.88/-680224410994.46 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000155\n",
      "155 reward -680564693277.08/-680561290473.79/-680224412948.08 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000156\n",
      "156 reward -680564693277.08/-680561290473.79/-680224412948.08 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000157\n",
      "157 reward -680564693277.08/-680557887651.78/-680224411076.37 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000158\n",
      "158 reward -680564693277.08/-680551082199.25/-679884148023.96 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000159\n",
      "159 reward -680564693277.08/-680534068272.35/-679884135172.48 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000160\n",
      "160 reward -680564693277.08/-680523859820.96/-679884135172.48 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000161\n",
      "161 reward -680564693277.08/-680503443089.93/-679543866642.21 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000162\n",
      "162 reward -680564693277.08/-680476220774.69/-679543866642.21 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000163\n",
      "163 reward -680564693277.08/-680442192961.15/-679543866642.21 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000164\n",
      "164 reward -680564693277.08/-680418373752.78/-679543866642.21 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000165\n",
      "165 reward -680564693277.08/-680370735272.11/-679543866642.21 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000166\n",
      "166 reward -680564693277.08/-680350318881.82/-679203598988.57 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000167\n",
      "167 reward -680564693277.08/-680289069288.95/-678863322115.30 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000168\n",
      "168 reward -680564693277.08/-680295875375.04/-678863322115.30 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000169\n",
      "169 reward -680564693277.08/-680333305608.91/-678863322115.30 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000170\n",
      "170 reward -680564693277.08/-680309486416.09/-678863322115.30 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000171\n",
      "171 reward -680564693277.08/-680343514236.86/-678863322115.30 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000172\n",
      "172 reward -680564693277.08/-680319695091.61/-678863322115.30 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000173\n",
      "173 reward -680564693277.08/-680336708563.88/-678523018579.09 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000174\n",
      "174 reward -680564693277.08/-680312889206.18/-678523018579.09 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000175\n",
      "175 reward -680564693277.08/-680275459354.47/-678523018579.09 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000176\n",
      "176 reward -680564693277.08/-680272056665.76/-678523018579.09 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000177\n",
      "177 reward -680564693277.08/-680306084159.10/-678523018579.09 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000178\n",
      "178 reward -680564693277.08/-680312889880.18/-678523018579.09 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000179\n",
      "179 reward -680564693277.08/-680272056265.17/-678523056470.97 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000180\n",
      "180 reward -680564693277.08/-675004222220.60/-149698020511.88 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000181\n",
      "181 reward -680564693277.08/-674987208005.45/-149698020511.88 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000182\n",
      "182 reward -680564693277.08/-674953180526.42/-149698020511.88 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000183\n",
      "183 reward -680564693277.08/-674953180250.87/-149698020511.88 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000184\n",
      "184 reward -680564693277.08/-674990611249.49/-149698020511.88 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000185\n",
      "185 reward -680564693277.08/-680309486236.81/-678182759932.67 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000186\n",
      "186 reward -680564693277.08/-680319694971.31/-679203583994.07 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000187\n",
      "187 reward -680564693277.08/-680258445463.77/-678182773098.65 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000188\n",
      "188 reward -680564693277.08/-674413169971.60/-96717674078.64 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000189\n",
      "189 reward -680564693277.08/-663470755687.22/-96717674078.64 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000190\n",
      "190 reward -680564693277.08/-663443533934.68/-96717674078.64 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000191\n",
      "191 reward -680564693277.08/-663436728515.24/-96717674078.64 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000192\n",
      "192 reward -680564693277.08/-663433325767.87/-96717674078.64 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000193\n",
      "193 reward -680564693277.08/-669333045389.48/-131126411173.29 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000194\n",
      "194 reward -680564693277.08/-669176597072.09/-115352865843.39 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000195\n",
      "195 reward -680564693277.08/-674640355429.84/-115352865843.39 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000196\n",
      "196 reward -680564693277.08/-674681188097.90/-115352865843.39 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000197\n",
      "197 reward -680564693277.08/-674694799392.47/-115352865843.39 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000198\n",
      "198 reward -680564693277.08/-674643758056.61/-115352865843.39 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000199\n",
      "199 reward -680564693277.08/-674596119567.56/-115352865843.39 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000200\n",
      "200 reward -680564693277.08/-680210807840.31/-678863317046.64 len 2000.00 saved C:\\Users\\Robert/ray_results\\A3C_gymFMU_2022-11-17_14-27-474dq1_4rb\\checkpoint_000201\n"
     ]
    }
   ],
   "source": [
    "N_ITER = 200\n",
    "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} saved {}\"\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent2.train()\n",
    "    file_name = agent2.save()\n",
    "\n",
    "    print(s.format(\n",
    "        n + 1,\n",
    "        result[\"episode_reward_min\"],\n",
    "        result[\"episode_reward_mean\"],\n",
    "        result[\"episode_reward_max\"],\n",
    "        result[\"episode_len_mean\"],\n",
    "        file_name\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0832fe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(10: ['obs', 'new_obs', 'actions', 'rewards', 'dones', 'infos', 't', 'eps_id', 'unroll_id', 'agent_index', 'vf_preds', 'advantages', 'value_targets', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(1: ['obs', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(1: ['obs', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(1: ['obs', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(1: ['obs', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(1: ['obs', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(1: ['obs', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(1: ['obs', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(1: ['obs', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(1: ['obs', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(1: ['obs', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m SampleBatch(1 (seqs=1): ['obs', 'obs_flat'])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16888)\u001b[0m calling value path\n"
     ]
    }
   ],
   "source": [
    "agent.training_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bb0b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60795c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': {'_hidden_layers.0._model.0.weight': array([[ 0.724345  , -0.689473  ],\n",
       "         [-0.2040475 , -0.97961926]], dtype=float32),\n",
       "  '_value_branch._model.0.weight': array([[ 0.05091035, -0.05270321]], dtype=float32)},\n",
       " 'global_timestep': 5880,\n",
       " 'policy_spec': {'policy_class': 'A3CTorchPolicy',\n",
       "  'observation_space': {'space': {'space': 'box',\n",
       "    'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGOpo6CrUKFAAuBoaG/yAMADdmHlc=',\n",
       "    'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGOpo6CrUKFAAuBoaGehAGADRmHVc=',\n",
       "    'shape': (2,),\n",
       "    'dtype': '<f4'}},\n",
       "  'action_space': {'space': {'space': 'box',\n",
       "    'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGOpo6CrUKFAAuBoaqIwDBahyW',\n",
       "    'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGOpo6CrUKFAAuBoYqFwDA6hwW',\n",
       "    'shape': (1,),\n",
       "    'dtype': '<f4'}},\n",
       "  'config': {'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'placement_strategy': 'PACK',\n",
       "   'eager_tracing': False,\n",
       "   'eager_max_retraces': 20,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'env': fmuSimulation.gymFMU.gymFMU,\n",
       "   'env_config': {'DEFAULT': {},\n",
       "    'General': {'DEBUG': False, 'LOG': False, 'Href': 10},\n",
       "    'FMU': {'fmuPath': '../../matlab/01-Watertank/rlwatertank.fmu',\n",
       "     'startTime': 0,\n",
       "     'stopTime': 200,\n",
       "     'tolerance': 1e-06,\n",
       "     'createFMU': False,\n",
       "     'dt': 0.1},\n",
       "    'Reinforcement Learning': {'actionInterval': 0.1},\n",
       "    'Environment': {},\n",
       "    'Reward': {}},\n",
       "   'observation_space': None,\n",
       "   'action_space': None,\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'clip_rewards': None,\n",
       "   'normalize_actions': True,\n",
       "   'clip_actions': False,\n",
       "   'disable_env_checking': False,\n",
       "   'num_workers': 1,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'sample_async': True,\n",
       "   'enable_connectors': False,\n",
       "   'rollout_fragment_length': 10,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'validate_workers_after_construction': True,\n",
       "   'ignore_worker_failures': False,\n",
       "   'recreate_failed_workers': False,\n",
       "   'restart_failed_sub_environments': False,\n",
       "   'num_consecutive_worker_failures_tolerance': 100,\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'compress_observations': False,\n",
       "   'enable_tf1_exec_eagerly': False,\n",
       "   'sampler_perf_stats_ema_coef': None,\n",
       "   'gamma': 0.99,\n",
       "   'lr': 0.0001,\n",
       "   'train_batch_size': 32,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    '_disable_preprocessor_api': False,\n",
       "    '_disable_action_flattening': False,\n",
       "    'fcnet_hiddens': [2],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': True,\n",
       "    'vf_share_layers': True,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'framestack': True,\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': ExampleTorch.FullyConnectedNetwork,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1},\n",
       "   'optimizer': {},\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'StochasticSampling'},\n",
       "   'input_config': {},\n",
       "   'actions_in_input_normalized': False,\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_config': {},\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_duration': 10,\n",
       "   'evaluation_duration_unit': 'episodes',\n",
       "   'evaluation_sample_timeout_s': 180.0,\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'evaluation_config': {},\n",
       "   'off_policy_estimation_methods': {},\n",
       "   'ope_split_batch_by_episode': True,\n",
       "   'evaluation_num_workers': 0,\n",
       "   'always_attach_evaluation_results': False,\n",
       "   'enable_async_evaluation': False,\n",
       "   'in_evaluation': False,\n",
       "   'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "   'keep_per_episode_custom_metrics': False,\n",
       "   'metrics_episode_collection_timeout_s': 60.0,\n",
       "   'metrics_num_episodes_for_smoothing': 100,\n",
       "   'min_time_s_per_iteration': 5,\n",
       "   'min_train_timesteps_per_iteration': 0,\n",
       "   'min_sample_timesteps_per_iteration': 0,\n",
       "   'export_native_model_files': False,\n",
       "   'logger_creator': None,\n",
       "   'logger_config': None,\n",
       "   'log_level': 'WARN',\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'seed': None,\n",
       "   '_tf_policy_handles_more_than_one_loss': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   '_disable_execution_plan_api': True,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'evaluation_num_episodes': -1,\n",
       "   'metrics_smoothing_episodes': -1,\n",
       "   'timesteps_per_iteration': -1,\n",
       "   'min_iter_time_s': -1,\n",
       "   'collect_metrics_timeout': -1,\n",
       "   'buffer_size': -1,\n",
       "   'prioritized_replay': -1,\n",
       "   'learning_starts': -1,\n",
       "   'replay_batch_size': -1,\n",
       "   'replay_sequence_length': None,\n",
       "   'replay_mode': -1,\n",
       "   'prioritized_replay_alpha': -1,\n",
       "   'prioritized_replay_beta': -1,\n",
       "   'prioritized_replay_eps': -1,\n",
       "   'min_time_s_per_reporting': -1,\n",
       "   'min_train_timesteps_per_reporting': -1,\n",
       "   'min_sample_timesteps_per_reporting': -1,\n",
       "   'input_evaluation': -1,\n",
       "   'use_critic': True,\n",
       "   'use_gae': True,\n",
       "   'grad_clip': 40.0,\n",
       "   'lr_schedule': None,\n",
       "   'vf_loss_coeff': 0.5,\n",
       "   'entropy_coeff': 0.01,\n",
       "   'entropy_coeff_schedule': None,\n",
       "   'lambda': 1.0,\n",
       "   'input': 'sampler',\n",
       "   'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec at 0x2321702f520>},\n",
       "    'policy_map_capacity': 100,\n",
       "    'policy_map_cache': None,\n",
       "    'policy_mapping_fn': None,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "   'create_env_on_driver': False,\n",
       "   'custom_eval_function': None,\n",
       "   'framework': 'torch',\n",
       "   'num_cpus_for_driver': 1,\n",
       "   'worker_index': 0}},\n",
       " '_optimizer_variables': [{'state': {0: {'step': 588.0,\n",
       "     'exp_avg': array([[-4.9636856e-10, -2.5424204e-10],\n",
       "            [-8.8517611e-09,  2.3286308e-09]], dtype=float32),\n",
       "     'exp_avg_sq': array([[0.35491973, 0.09344816],\n",
       "            [1.832915  , 0.11952338]], dtype=float32)},\n",
       "    1: {'step': 588.0,\n",
       "     'exp_avg': array([[-28.284248,  28.284248]], dtype=float32),\n",
       "     'exp_avg_sq': array([[354.78964, 354.37094]], dtype=float32)}},\n",
       "   'param_groups': [{'lr': 0.0001,\n",
       "     'betas': (0.9, 0.999),\n",
       "     'eps': 1e-08,\n",
       "     'weight_decay': 0,\n",
       "     'amsgrad': False,\n",
       "     'maximize': False,\n",
       "     'foreach': None,\n",
       "     'capturable': False,\n",
       "     'params': [0, 1]}]}],\n",
       " '_exploration_state': {}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent2.get_policy().get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e5274c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': {'_hidden_layers.0._model.0.weight': array([[ 0.8568847 ,  0.5171064 ],\n",
       "         [-0.41770497,  0.9075769 ]], dtype=float32),\n",
       "  '_value_branch._model.0.weight': array([[0.06013679, 0.06109789]], dtype=float32)},\n",
       " 'global_timestep': 5360,\n",
       " 'policy_spec': {'policy_class': 'A3CTorchPolicy',\n",
       "  'observation_space': {'space': {'space': 'box',\n",
       "    'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGOpo6CrUKFAAuBoaG/yAMADdmHlc=',\n",
       "    'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGOpo6CrUKFAAuBoaGehAGADRmHVc=',\n",
       "    'shape': (2,),\n",
       "    'dtype': '<f4'}},\n",
       "  'action_space': {'space': {'space': 'box',\n",
       "    'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGOpo6CrUKFAAuBoaqIwDBahyW',\n",
       "    'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGOpo6CrUKFAAuBoYqFwDA6hwW',\n",
       "    'shape': (1,),\n",
       "    'dtype': '<f4'}},\n",
       "  'config': {'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'placement_strategy': 'PACK',\n",
       "   'eager_tracing': False,\n",
       "   'eager_max_retraces': 20,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'env': fmuSimulation.gymFMU.gymFMU,\n",
       "   'env_config': {'DEFAULT': {},\n",
       "    'General': {'DEBUG': False, 'LOG': False, 'Href': 10},\n",
       "    'FMU': {'fmuPath': '../../matlab/01-Watertank/rlwatertank.fmu',\n",
       "     'startTime': 0,\n",
       "     'stopTime': 200,\n",
       "     'tolerance': 1e-06,\n",
       "     'createFMU': False,\n",
       "     'dt': 0.1},\n",
       "    'Reinforcement Learning': {'actionInterval': 0.1},\n",
       "    'Environment': {},\n",
       "    'Reward': {}},\n",
       "   'observation_space': None,\n",
       "   'action_space': None,\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'clip_rewards': None,\n",
       "   'normalize_actions': True,\n",
       "   'clip_actions': False,\n",
       "   'disable_env_checking': False,\n",
       "   'num_workers': 1,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'sample_async': True,\n",
       "   'enable_connectors': False,\n",
       "   'rollout_fragment_length': 10,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'validate_workers_after_construction': True,\n",
       "   'ignore_worker_failures': False,\n",
       "   'recreate_failed_workers': False,\n",
       "   'restart_failed_sub_environments': False,\n",
       "   'num_consecutive_worker_failures_tolerance': 100,\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'compress_observations': False,\n",
       "   'enable_tf1_exec_eagerly': False,\n",
       "   'sampler_perf_stats_ema_coef': None,\n",
       "   'gamma': 0.99,\n",
       "   'lr': 0.0001,\n",
       "   'train_batch_size': 32,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    '_disable_preprocessor_api': False,\n",
       "    '_disable_action_flattening': False,\n",
       "    'fcnet_hiddens': [2],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': True,\n",
       "    'vf_share_layers': True,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'framestack': True,\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': ExampleTorch.FullyConnectedNetwork,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1},\n",
       "   'optimizer': {},\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'StochasticSampling'},\n",
       "   'input_config': {},\n",
       "   'actions_in_input_normalized': False,\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_config': {},\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_duration': 10,\n",
       "   'evaluation_duration_unit': 'episodes',\n",
       "   'evaluation_sample_timeout_s': 180.0,\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'evaluation_config': {},\n",
       "   'off_policy_estimation_methods': {},\n",
       "   'ope_split_batch_by_episode': True,\n",
       "   'evaluation_num_workers': 0,\n",
       "   'always_attach_evaluation_results': False,\n",
       "   'enable_async_evaluation': False,\n",
       "   'in_evaluation': False,\n",
       "   'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "   'keep_per_episode_custom_metrics': False,\n",
       "   'metrics_episode_collection_timeout_s': 60.0,\n",
       "   'metrics_num_episodes_for_smoothing': 100,\n",
       "   'min_time_s_per_iteration': 5,\n",
       "   'min_train_timesteps_per_iteration': 0,\n",
       "   'min_sample_timesteps_per_iteration': 0,\n",
       "   'export_native_model_files': False,\n",
       "   'logger_creator': None,\n",
       "   'logger_config': None,\n",
       "   'log_level': 'WARN',\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'seed': None,\n",
       "   '_tf_policy_handles_more_than_one_loss': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   '_disable_execution_plan_api': True,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'evaluation_num_episodes': -1,\n",
       "   'metrics_smoothing_episodes': -1,\n",
       "   'timesteps_per_iteration': -1,\n",
       "   'min_iter_time_s': -1,\n",
       "   'collect_metrics_timeout': -1,\n",
       "   'buffer_size': -1,\n",
       "   'prioritized_replay': -1,\n",
       "   'learning_starts': -1,\n",
       "   'replay_batch_size': -1,\n",
       "   'replay_sequence_length': None,\n",
       "   'replay_mode': -1,\n",
       "   'prioritized_replay_alpha': -1,\n",
       "   'prioritized_replay_beta': -1,\n",
       "   'prioritized_replay_eps': -1,\n",
       "   'min_time_s_per_reporting': -1,\n",
       "   'min_train_timesteps_per_reporting': -1,\n",
       "   'min_sample_timesteps_per_reporting': -1,\n",
       "   'input_evaluation': -1,\n",
       "   'use_critic': True,\n",
       "   'use_gae': True,\n",
       "   'grad_clip': 40.0,\n",
       "   'lr_schedule': None,\n",
       "   'vf_loss_coeff': 0.5,\n",
       "   'entropy_coeff': 0.01,\n",
       "   'entropy_coeff_schedule': None,\n",
       "   'lambda': 1.0,\n",
       "   'input': 'sampler',\n",
       "   'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec at 0x23216eb5640>},\n",
       "    'policy_map_capacity': 100,\n",
       "    'policy_map_cache': None,\n",
       "    'policy_mapping_fn': None,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "   'create_env_on_driver': False,\n",
       "   'custom_eval_function': None,\n",
       "   'framework': 'torch',\n",
       "   'num_cpus_for_driver': 1,\n",
       "   'worker_index': 0}},\n",
       " '_optimizer_variables': [{'state': {0: {'step': 536.0,\n",
       "     'exp_avg': array([[8.8941533e-08, 3.6639538e-08],\n",
       "            [2.8945540e-06, 1.7461024e-06]], dtype=float32),\n",
       "     'exp_avg_sq': array([[0.00801774, 0.00337395],\n",
       "            [2.494075  , 0.70544904]], dtype=float32)},\n",
       "    1: {'step': 536.0,\n",
       "     'exp_avg': array([[-28.284248, -28.284248]], dtype=float32),\n",
       "     'exp_avg_sq': array([[330.43893, 330.46704]], dtype=float32)}},\n",
       "   'param_groups': [{'lr': 0.0001,\n",
       "     'betas': (0.9, 0.999),\n",
       "     'eps': 1e-08,\n",
       "     'weight_decay': 0,\n",
       "     'amsgrad': False,\n",
       "     'maximize': False,\n",
       "     'foreach': None,\n",
       "     'capturable': False,\n",
       "     'params': [0, 1]}]}],\n",
       " '_exploration_state': {}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent2.get_policy().get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7e15144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " observations (InputLayer)      [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " fc_value_1 (Dense)             (None, 2)            6           ['observations[0][0]']           \n",
      "                                                                                                  \n",
      " fc_out (Dense)                 (None, 2)            6           ['observations[0][0]']           \n",
      "                                                                                                  \n",
      " value_out (Dense)              (None, 1)            3           ['fc_value_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.get_policy().model.base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3ab1f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default_policy': {'_hidden_layers.0._model.0.weight': array([[ 0.5535853, -0.8327925],\n",
       "         [-0.3591717, -0.9332716]], dtype=float32),\n",
       "  '_hidden_layers.0._model.0.bias': array([0., 0.], dtype=float32),\n",
       "  '_value_branch_separate.0._model.0.weight': array([[ 0.27822936,  0.9605147 ],\n",
       "         [-0.7842211 , -0.6204815 ]], dtype=float32),\n",
       "  '_value_branch_separate.0._model.0.bias': array([0., 0.], dtype=float32),\n",
       "  '_value_branch._model.0.weight': array([[ 0.00977398, -0.00211409]], dtype=float32),\n",
       "  '_value_branch._model.0.bias': array([0.], dtype=float32)}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c604a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fmuPath': '../../matlab/01-Watertank/rlwatertank.fmu',\n",
       " 'startTime': 0,\n",
       " 'stopTime': 10,\n",
       " 'tolerance': 1e-06,\n",
       " 'createFMU': False,\n",
       " 'dt': 0.1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['FMU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d954a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ExampleFMU.gymFMU(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3c84fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a5c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7daab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "23431728c7532b37a33913d127f36cf7c0ea2ff8bac737dbe1e9eccff71e7bd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
